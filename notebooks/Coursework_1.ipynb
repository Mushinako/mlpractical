{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import TYPE_CHECKING, Any, Callable\n",
    "\n",
    "import logging\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from matplotlib.axes._subplots import AxesSubplot\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    _1DArray = np.ndarray[tuple[int], np.dtype[np.float64]]\n",
    "    _2DArray = np.ndarray[tuple[int, int], np.dtype[np.float64]]\n",
    "    _NDArray = np.ndarray[tuple[int, ...], np.dtype[np.float64]]\n",
    "    _TrainResult = tuple[_2DArray, dict[str, int], float]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.data_providers import DataProvider, EMNISTDataProvider, MNISTDataProvider\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.layers import AffineLayer, DropoutLayer, ReluLayer, SigmoidLayer, SoftmaxLayer\n",
    "from mlp.learning_rules import AdamLearningRule, GradientDescentLearningRule\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.optimisers import Optimiser\n",
    "from mlp.penalties import L1Penalty, L2Penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up RNG, Logger, and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 11102019\n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider(\"train\", batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider(\"valid\", batch_size=batch_size, rng=rng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Metavariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "# setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "# input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "# weights_init = GlorotUniformInit(rng=rng)\n",
    "# biases_init = ConstantInit(0.)\n",
    "# model = MultipleLayerModel([\n",
    "#     AffineLayer(input_dim, hidden_dim, weights_init, biases_init),\n",
    "#     ReluLayer(),\n",
    "#     AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "#     ReluLayer(),\n",
    "#     AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "# ])\n",
    "\n",
    "# error = CrossEntropySoftmaxError()\n",
    "# # Use a basic gradient descent learning rule\n",
    "# learning_rule = AdamLearningRule()\n",
    "\n",
    "# #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "# _ = train_model_and_plot_stats(\n",
    "#     model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Heavy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hidden_dim = None\n",
    "data_hidden_layer = None\n",
    "\n",
    "data_dropout = None\n",
    "data_l1 = None\n",
    "data_l2 = None\n",
    "\n",
    "data_combined_1 = None\n",
    "data_combined_2 = None\n",
    "data_combined_3 = None\n",
    "data_combined_4 = None\n",
    "data_combined_5 = None\n",
    "data_combined_6 = None\n",
    "data_combined_7 = None\n",
    "data_combined_8 = None\n",
    "\n",
    "data_dropout_slow = None\n",
    "data_l1_slow = None\n",
    "data_l2_slow = None\n",
    "\n",
    "data_combined_slow_1 = None\n",
    "data_combined_slow_2 = None\n",
    "data_combined_slow_3 = None\n",
    "data_combined_slow_4 = None\n",
    "data_combined_slow_5 = None\n",
    "data_combined_slow_6 = None\n",
    "\n",
    "%store -r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(\n",
    "    hidden_dim: int,\n",
    "    hidden_layer_count: int,\n",
    "    *,\n",
    "    learning_rate: float = 1e-3,\n",
    "    dropout: None | Callable[[], DropoutLayer] = None,\n",
    "    penalty: Callable[[], None | L1Penalty | L2Penalty] = lambda: None,\n",
    ") -> _TrainResult:\n",
    "    \"\"\"\n",
    "    Run different models specified in the question.\n",
    "\n",
    "    Args:\n",
    "        hidden_dim (int):\n",
    "            Hidden layer dimension\n",
    "        hidden_layer_count (int):\n",
    "            Number of hidden layers. At least 1 hidden layer is required\n",
    "        learning_rate (float):\n",
    "            Learning rate. Default 1e-3\n",
    "        dropout (None | (() -> DropoutLayer)):\n",
    "            Dropout layer constructor. If `None`, no dropout layers are\n",
    "            included\n",
    "        penalty (() -> (None | L1Penalty | L2Penalty)):\n",
    "            Penalty constructor. If the constructor returns `None`, no penalty\n",
    "            is applied\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray):\n",
    "            2D array of statistics\n",
    "        (dict[str, int]):\n",
    "            Key-index mapping of all the indices\n",
    "        (float):\n",
    "            Run time for this model\n",
    "    \"\"\"\n",
    "    if hidden_layer_count < 1:\n",
    "        raise ValueError(\"There must be at least one hidden layer\")\n",
    "\n",
    "    input_dim, output_dim = 784, 47\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.0)\n",
    "\n",
    "    layers: list[object] = [\n",
    "        AffineLayer(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            weights_init,\n",
    "            biases_init,\n",
    "            weights_penalty=penalty(),\n",
    "            biases_penalty=penalty(),\n",
    "        ),\n",
    "        ReluLayer(),\n",
    "    ]\n",
    "    if dropout is not None:\n",
    "        layers.append(dropout())\n",
    "    for _ in range(hidden_layer_count - 1):\n",
    "        layers += [\n",
    "            AffineLayer(\n",
    "                hidden_dim,\n",
    "                hidden_dim,\n",
    "                weights_init,\n",
    "                biases_init,\n",
    "                weights_penalty=penalty(),\n",
    "                biases_penalty=penalty(),\n",
    "            ),\n",
    "            ReluLayer(),\n",
    "        ]\n",
    "        if dropout is not None:\n",
    "            layers.append(dropout())\n",
    "    layers.append(\n",
    "        AffineLayer(\n",
    "            hidden_dim,\n",
    "            output_dim,\n",
    "            weights_init,\n",
    "            biases_init,\n",
    "            weights_penalty=penalty(),\n",
    "            biases_penalty=penalty(),\n",
    "        )\n",
    "    )\n",
    "    model = MultipleLayerModel(layers)\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    return train_model_and_plot_stats(\n",
    "        model,\n",
    "        error,\n",
    "        learning_rule,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        num_epochs,\n",
    "        stats_interval,\n",
    "        notebook=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models and Plot Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_plot_stats(\n",
    "    model: MultipleLayerModel,\n",
    "    error: CrossEntropySoftmaxError,\n",
    "    learning_rule: GradientDescentLearningRule,\n",
    "    train_data: DataProvider,\n",
    "    valid_data: DataProvider,\n",
    "    num_epochs: int,\n",
    "    stats_interval: int,\n",
    "    notebook: bool = True,\n",
    ") -> _TrainResult:\n",
    "    \"\"\"\n",
    "    Train model and plot related statistics.\n",
    "\n",
    "    Args:\n",
    "        model (MultipleLayerModel):\n",
    "            A model that is to be trained\n",
    "        error (CrossEntropySoftmaxError):\n",
    "            Error calculator\n",
    "        learning_rule (GradientDescentLearningRule):\n",
    "            Learning rule\n",
    "        train_data (DataProvider):\n",
    "            Data to be used for training\n",
    "        valid_data (DataProvider):\n",
    "            Data to be used for validation\n",
    "        num_epochs (int):\n",
    "            Number of epochs\n",
    "        stats_interval (int):\n",
    "            How often training stats are recorded. Stats are recorded every\n",
    "            `stats_interval` epochs\n",
    "        notebook (bool):\n",
    "            Whether the code is run in a Jupyter notebook. Default `True`\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray):\n",
    "            2D array of statistics\n",
    "        (dict[str, int]):\n",
    "            Key-index mapping of all the indices\n",
    "        (float):\n",
    "            Run time for this model\n",
    "    \"\"\"\n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors: dict[str, Callable[[_NDArray, _NDArray], float]] = {\n",
    "        \"acc\": lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()\n",
    "    }\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model,\n",
    "        error,\n",
    "        learning_rule,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        data_monitors,\n",
    "        notebook=notebook,\n",
    "    )\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats: _2DArray\n",
    "    keys: dict[str, int]\n",
    "    stats, keys, run_time = optimiser.train(\n",
    "        num_epochs=num_epochs, stats_interval=stats_interval\n",
    "    )\n",
    "\n",
    "    # # Plot the change in the validation and training set error over training.\n",
    "    # fig_1 = plt.figure(figsize=(8, 4))\n",
    "    # ax_1 = fig_1.add_subplot(111)\n",
    "    # for k in ['error(train)', 'error(valid)']:\n",
    "    #     ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval,\n",
    "    #               stats[1:, keys[k]], label=k)\n",
    "    # ax_1.legend(loc=0)\n",
    "    # ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # # Plot the change in the validation and training set accuracy over training.\n",
    "    # fig_2 = plt.figure(figsize=(8, 4))\n",
    "    # ax_2 = fig_2.add_subplot(111)\n",
    "    # for k in ['acc(train)', 'acc(valid)']:\n",
    "    #     ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval,\n",
    "    #               stats[1:, keys[k]], label=k)\n",
    "    # ax_2.legend(loc=0)\n",
    "    # ax_2.set_xlabel('Epoch number')\n",
    "\n",
    "    # return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2\n",
    "    return stats, keys, run_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pretty Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_stat_str(data: _TrainResult) -> str:\n",
    "    \"\"\"\n",
    "    Generate stat string.\n",
    "    \"\"\"\n",
    "    stats, keys, *_ = data\n",
    "    # Only print the first time\n",
    "    sorted_keys = sorted(keys.items(), key=itemgetter(1))\n",
    "    return \", \".join(f\"{k[0]}={v:.2e}\" for (k, v) in zip(sorted_keys, stats[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Gap String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_acc_gap_str(data: _TrainResult) -> str:\n",
    "    \"\"\"\n",
    "    Generate accuracy and gap string.\n",
    "    \"\"\"\n",
    "    stat: _1DArray = data[0][-1]\n",
    "    keys = data[1]\n",
    "    acc = stat[keys[\"acc(valid)\"]]\n",
    "    gap = stat[keys[\"error(valid)\"]] - stat[keys[\"error(train)\"]]\n",
    "    return f\"Accuracy {acc:.2e}, Gap {gap:.2e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Figure 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig_2_3(data: dict[int, _TrainResult], prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot utility function for Fig. 2 and 3.\n",
    "    \"\"\"\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "\n",
    "    for i, (t, linestyle) in enumerate({\"(train)\": \"-\", \"(valid)\": \"--\"}.items()):\n",
    "        for w, datum in data.items():\n",
    "            stats, keys, *_ = datum\n",
    "            if not i:\n",
    "                # Only print the first time\n",
    "                print(f\"{w}: {gen_stat_str(datum)}\")\n",
    "            ax_1.plot(\n",
    "                np.arange(1, stats.shape[0]) * stats_interval,\n",
    "                stats[1:, keys[f\"error{t}\"]],\n",
    "                label=f\"{prefix} {w}{t}\",\n",
    "                linestyle=linestyle,\n",
    "            )\n",
    "            ax_2.plot(\n",
    "                np.arange(1, stats.shape[0]) * stats_interval,\n",
    "                stats[1:, keys[f\"acc{t}\"]],\n",
    "                label=f\"{prefix} {w}{t}\",\n",
    "                linestyle=linestyle,\n",
    "            )\n",
    "\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel(\"Epoch number\")\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel(\"Epoch number\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Figure 4 for Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig_4_dropout(data: dict[float, _TrainResult], learning_rate: float) -> None:\n",
    "    \"\"\"\n",
    "    Plot utility function for Fig. 4 dropout.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    accs: list[float] = []\n",
    "    gaps: list[float] = []\n",
    "    for dropout, datum in data.items():\n",
    "        print(\n",
    "            f\"Dropout {dropout} with learning rate {learning_rate}: \"\n",
    "            f\"{gen_acc_gap_str(datum)}\"\n",
    "        )\n",
    "        stat: _1DArray = datum[0][-1]\n",
    "        keys = datum[1]\n",
    "        accs.append(stat[keys[\"acc(valid)\"]])\n",
    "        gaps.append(stat[keys[\"error(valid)\"]] - stat[keys[\"error(train)\"]])\n",
    "\n",
    "    l1 = ax1.plot(data.keys(), accs, \"r\", label=\"Val. Acc.\")\n",
    "    l2 = ax2.plot(data.keys(), gaps, \"b\", label=\"Gap\")\n",
    "    lines = l1 + l2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "\n",
    "    ax1.legend(lines, labels, loc=0)\n",
    "    ax1.set_xlabel(\"Dropout value\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Generalization gap\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Figure 4 for Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig_4_penalty_l(\n",
    "    data: dict[float, _TrainResult],\n",
    "    type_: str,\n",
    "    learning_rate: float,\n",
    "    axes: tuple[AxesSubplot, AxesSubplot],\n",
    ") -> tuple[list[Line2D], list[Line2D]]:\n",
    "    \"\"\"\n",
    "    Plot utility function for Fig.4 penalty for one of L1/L2.\n",
    "    \"\"\"\n",
    "    accs: list[float] = []\n",
    "    gaps: list[float] = []\n",
    "    for value, datum in data.items():\n",
    "        print(\n",
    "            f\"{type_} {value} with learning rate {learning_rate}: \"\n",
    "            f\"{gen_acc_gap_str(datum)}\"\n",
    "        )\n",
    "        stat: _1DArray = datum[0][-1]\n",
    "        keys = datum[1]\n",
    "        accs.append(stat[keys[\"acc(valid)\"]])\n",
    "        gaps.append(stat[keys[\"error(valid)\"]] - stat[keys[\"error(train)\"]])\n",
    "\n",
    "    ax1, ax2 = axes\n",
    "    acc = ax1.plot(data.keys(), accs, label=f\"{type_} Val. Acc.\")\n",
    "    gap = ax2.plot(data.keys(), gaps, label=f\"{type_} Gap\")\n",
    "    return acc, gap\n",
    "\n",
    "\n",
    "def plot_fig_4_penalty(\n",
    "    data_l1: dict[float, _TrainResult],\n",
    "    data_l2: dict[float, _TrainResult],\n",
    "    learning_rate: float,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot utility function for Fig. 4 penalty.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    l1_acc, l1_gap = plot_fig_4_penalty_l(data_l1, \"L1\", learning_rate, (ax1, ax2))\n",
    "    l2_acc, l2_gap = plot_fig_4_penalty_l(data_l2, \"L2\", learning_rate, (ax1, ax2))\n",
    "\n",
    "    lines = l1_acc + l1_gap + l2_acc + l2_gap\n",
    "    labels = [l.get_label() for l in lines]\n",
    "\n",
    "    ax1.legend(lines, labels, loc=0)\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.set_xlabel(\"Penalty value\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Generalization gap\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Validation Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig_valid(data: dict[float, _TrainResult], prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot utility function accuracy and error stats for validation data.\n",
    "    \"\"\"\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "\n",
    "    for w, datum in data.items():\n",
    "        stats, keys, *_ = datum\n",
    "        # Only print the first time\n",
    "        print(f\"{w}: {gen_stat_str(datum)}\")\n",
    "        ax_1.plot(\n",
    "            np.arange(1, stats.shape[0]) * stats_interval,\n",
    "            stats[1:, keys[\"error(valid)\"]],\n",
    "            label=f\"{prefix} {w}(valid)\",\n",
    "        )\n",
    "        ax_2.plot(\n",
    "            np.arange(1, stats.shape[0]) * stats_interval,\n",
    "            stats[1:, keys[\"acc(valid)\"]],\n",
    "            label=f\"{prefix} {w}(valid)\",\n",
    "        )\n",
    "\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel(\"Epoch number\")\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel(\"Epoch number\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying Hidden Layer Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_hidden_dim is None:\n",
    "    data_hidden_dim = {\n",
    "        32: run_model(32, 1),\n",
    "        64: run_model(64, 1),\n",
    "        128: run_model(128, 1),\n",
    "    }\n",
    "    %store data_hidden_dim\n",
    "for dim, data in data_hidden_dim.items():\n",
    "    print(f\"Dimension {dim} run time: {data[2]:.3f} seconds\")\n",
    "plot_fig_2_3(data_hidden_dim, \"width\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying the Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_hidden_layer is None:\n",
    "    data_hidden_layer = {\n",
    "        1: data_hidden_dim[128],\n",
    "        2: run_model(128, 2),\n",
    "        3: run_model(128, 3),\n",
    "    }\n",
    "    %store data_hidden_layer\n",
    "for layer, data in data_hidden_layer.items():\n",
    "    print(f\"Layer {layer} run time: {data[2]:.3f} seconds\")\n",
    "plot_fig_2_3(data_hidden_layer, \"depth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_dropout is None:\n",
    "    data_dropout = {\n",
    "        prob: run_model(128, 3, dropout=lambda: DropoutLayer(incl_prob=prob))\n",
    "        for prob in [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    %store data_dropout\n",
    "for prob, data in data_dropout.items():\n",
    "    print(\n",
    "        f\"Dropout probability {prob} with 1e-3 learning rate run time: \"\n",
    "        f\"{data[2]:.3f} seconds\"\n",
    "    )\n",
    "plot_fig_4_dropout(data_dropout, 1e-3)\n",
    "plot_fig_valid(data_dropout, \"dropout\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_l1 is None:\n",
    "    data_l1 = {\n",
    "        pen: run_model(128, 3, penalty=lambda: L1Penalty(pen))\n",
    "        for pen in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "    %store data_l1\n",
    "for pen, data in data_l1.items():\n",
    "    print(f\"L1 penalty {pen} run time: {data[2]:.3f} seconds\")\n",
    "\n",
    "if data_l2 is None:\n",
    "    data_l2 = {\n",
    "        pen: run_model(128, 3, penalty=lambda: L2Penalty(pen))\n",
    "        for pen in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "    %store data_l2\n",
    "for pen, data in data_l2.items():\n",
    "    print(f\"L2 penalty {pen} run time: {data[2]:.3f} seconds\")\n",
    "\n",
    "plot_fig_4_penalty(data_l1, data_l2, 1e-3)\n",
    "plot_fig_valid(data_l1, \"l1\")\n",
    "plot_fig_valid(data_l2, \"l2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_1 is None:\n",
    "    data_combined_1 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.5),\n",
    "        penalty=lambda: L1Penalty(1e-4),\n",
    "    )\n",
    "    %store data_combined_1\n",
    "data_combined_1_name = \"Dropout 0.5 & L1 1e-4\"\n",
    "print(f\"{data_combined_1_name}: Run time {data_combined_1[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_1_name}: {gen_acc_gap_str(data_combined_1)}\")\n",
    "print(f\"{data_combined_1_name}: {gen_stat_str(data_combined_1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_2 is None:\n",
    "    data_combined_2 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.5),\n",
    "        penalty=lambda: L2Penalty(1e-3),\n",
    "    )\n",
    "    %store data_combined_2\n",
    "data_combined_2_name = \"Dropout 0.5 & L2 1e-3\"\n",
    "print(f\"{data_combined_2_name}: Run time {data_combined_2[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_2_name}: {gen_acc_gap_str(data_combined_2)}\")\n",
    "print(f\"{data_combined_2_name}: {gen_stat_str(data_combined_2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_3 is None:\n",
    "    data_combined_3 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.7),\n",
    "        penalty=lambda: L1Penalty(1e-4),\n",
    "    )\n",
    "    %store data_combined_3\n",
    "data_combined_3_name = \"Dropout 0.7 & L1 1e-4\"\n",
    "print(f\"{data_combined_3_name}: Run time {data_combined_3[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_3_name}: {gen_acc_gap_str(data_combined_3)}\")\n",
    "print(f\"{data_combined_3_name}: {gen_stat_str(data_combined_3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_4 is None:\n",
    "    data_combined_4 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.7),\n",
    "        penalty=lambda: L2Penalty(1e-3),\n",
    "    )\n",
    "    %store data_combined_4\n",
    "data_combined_4_name = \"Dropout 0.7 & L2 1e-3\"\n",
    "print(f\"{data_combined_4_name}: Run time {data_combined_4[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_4_name}: {gen_acc_gap_str(data_combined_4)}\")\n",
    "print(f\"{data_combined_4_name}: {gen_stat_str(data_combined_4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_5 is None:\n",
    "    data_combined_5 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.5),\n",
    "        penalty=lambda: L1Penalty(1e-5),\n",
    "    )\n",
    "    %store data_combined_5\n",
    "data_combined_5_name = \"Dropout 0.5 & L1 1e-5\"\n",
    "print(f\"{data_combined_5_name}: Run time {data_combined_5[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_5_name}: {gen_acc_gap_str(data_combined_5)}\")\n",
    "print(f\"{data_combined_5_name}: {gen_stat_str(data_combined_5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_6 is None:\n",
    "    data_combined_6 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.5),\n",
    "        penalty=lambda: L2Penalty(1e-4),\n",
    "    )\n",
    "    %store data_combined_6\n",
    "data_combined_6_name = \"Dropout 0.5 & L2 1e-4\"\n",
    "print(f\"{data_combined_6_name}: Run time {data_combined_6[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_6_name}: {gen_acc_gap_str(data_combined_6)}\")\n",
    "print(f\"{data_combined_6_name}: {gen_stat_str(data_combined_6)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_7 is None:\n",
    "    data_combined_7 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.7),\n",
    "        penalty=lambda: L1Penalty(1e-5),\n",
    "    )\n",
    "    %store data_combined_7\n",
    "data_combined_7_name = \"Dropout 0.7 & L1 1e-5\"\n",
    "print(f\"{data_combined_7_name}: Run time {data_combined_7[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_7_name}: {gen_acc_gap_str(data_combined_7)}\")\n",
    "print(f\"{data_combined_7_name}: {gen_stat_str(data_combined_7)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_8 is None:\n",
    "    data_combined_8 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.7),\n",
    "        penalty=lambda: L2Penalty(1e-4),\n",
    "    )\n",
    "    %store data_combined_8\n",
    "data_combined_8_name = \"Dropout 0.5 & L2 1e-4\"\n",
    "print(f\"{data_combined_8_name}: Run time {data_combined_8[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_8_name}: {gen_acc_gap_str(data_combined_8)}\")\n",
    "print(f\"{data_combined_8_name}: {gen_stat_str(data_combined_8)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e-4 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_dropout_slow is None:\n",
    "    data_dropout_slow = {\n",
    "        prob: run_model(\n",
    "            128, 3, learning_rate=1e-4, dropout=lambda: DropoutLayer(incl_prob=prob)\n",
    "        )\n",
    "        for prob in [0.7, 0.9, 0.95]\n",
    "    }\n",
    "    %store data_dropout_slow\n",
    "for prob, data in data_dropout_slow.items():\n",
    "    print(\n",
    "        \"Dropout probability {prob} with 1e-4 learning rate run time: \"\n",
    "        f\"{data[2]:.3f} seconds\"\n",
    "    )\n",
    "plot_fig_4_dropout(data_dropout_slow, 1e-4)\n",
    "plot_fig_valid(data_dropout_slow, \"dropout\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_l1_slow is None:\n",
    "    data_l1_slow = {\n",
    "        pen: run_model(128, 3, learning_rate=1e-4, penalty=lambda: L1Penalty(pen))\n",
    "        for pen in [1e-4, 1e-3, 1e-1]\n",
    "    }\n",
    "    %store data_l1_slow\n",
    "for pen, data in data_l1_slow.items():\n",
    "    print(f\"L1 penalty {pen} with 1e-4 learning rate run time: {data[2]:.3f} seconds\")\n",
    "\n",
    "if data_l2_slow is None:\n",
    "    data_l2_slow = {\n",
    "        pen: run_model(128, 3, learning_rate=1e-4, penalty=lambda: L2Penalty(pen))\n",
    "        for pen in [1e-4, 1e-3, 1e-1]\n",
    "    }\n",
    "    %store data_l2_slow\n",
    "for pen, data in data_l2_slow.items():\n",
    "    print(f\"L2 penalty {pen} with 1e-4 learning rate run time: {data[2]:.3f} seconds\")\n",
    "\n",
    "plot_fig_4_penalty(data_l1_slow, data_l2_slow, 1e-4)\n",
    "plot_fig_valid(data_l1_slow, \"l1\")\n",
    "plot_fig_valid(data_l2_slow, \"l2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_slow_1 is None:\n",
    "    data_combined_slow_1 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        learning_rate=1e-4,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.7),\n",
    "        penalty=lambda: L1Penalty(1e-4),\n",
    "    )\n",
    "    %store data_combined_slow_1\n",
    "data_combined_slow_1_name = \"Dropout 0.7 & L1 1e-4 with 1e-4 learning rate\"\n",
    "print(f\"{data_combined_slow_1_name}: Run time {data_combined_slow_1[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_slow_1_name}: {gen_acc_gap_str(data_combined_slow_1)}\")\n",
    "print(f\"{data_combined_slow_1_name}: {gen_stat_str(data_combined_slow_1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_slow_2 is None:\n",
    "    data_combined_slow_2 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        learning_rate=1e-4,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.7),\n",
    "        penalty=lambda: L1Penalty(1e-3),\n",
    "    )\n",
    "    %store data_combined_slow_2\n",
    "data_combined_slow_2_name = \"Dropout 0.7 & L1 1e-3 with 1e-4 learning rate\"\n",
    "print(f\"{data_combined_slow_2_name}: Run time {data_combined_slow_2[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_slow_2_name}: {gen_acc_gap_str(data_combined_slow_2)}\")\n",
    "print(f\"{data_combined_slow_2_name}: {gen_stat_str(data_combined_slow_2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_slow_3 is None:\n",
    "    data_combined_slow_3 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        learning_rate=1e-4,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.7),\n",
    "        penalty=lambda: L2Penalty(1e-3),\n",
    "    )\n",
    "    %store data_combined_slow_3\n",
    "data_combined_slow_3_name = \"Dropout 0.7 & L2 1e-3 with 1e-4 learning rate\"\n",
    "print(f\"{data_combined_slow_3_name}: Run time {data_combined_slow_3[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_slow_3_name}: {gen_acc_gap_str(data_combined_slow_3)}\")\n",
    "print(f\"{data_combined_slow_3_name}: {gen_stat_str(data_combined_slow_3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_slow_4 is None:\n",
    "    data_combined_slow_4 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        learning_rate=1e-4,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.9),\n",
    "        penalty=lambda: L1Penalty(1e-4),\n",
    "    )\n",
    "    %store data_combined_slow_4\n",
    "data_combined_slow_4_name = \"Dropout 0.9 & L1 1e-4 with 1e-4 learning rate\"\n",
    "print(f\"{data_combined_slow_4_name}: Run time {data_combined_slow_4[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_slow_4_name}: {gen_acc_gap_str(data_combined_slow_4)}\")\n",
    "print(f\"{data_combined_slow_4_name}: {gen_stat_str(data_combined_slow_4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_slow_5 is None:\n",
    "    data_combined_slow_5 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        learning_rate=1e-4,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.9),\n",
    "        penalty=lambda: L1Penalty(1e-3),\n",
    "    )\n",
    "    %store data_combined_slow_5\n",
    "data_combined_slow_5_name = \"Dropout 0.9 & L1 1e-3 with 1e-4 learning rate\"\n",
    "print(f\"{data_combined_slow_5_name}: Run time {data_combined_slow_5[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_slow_5_name}: {gen_acc_gap_str(data_combined_slow_5)}\")\n",
    "print(f\"{data_combined_slow_5_name}: {gen_stat_str(data_combined_slow_5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_slow_6 is None:\n",
    "    data_combined_slow_6 = run_model(\n",
    "        128,\n",
    "        3,\n",
    "        learning_rate=1e-4,\n",
    "        dropout=lambda: DropoutLayer(incl_prob=0.9),\n",
    "        penalty=lambda: L2Penalty(1e-3),\n",
    "    )\n",
    "    %store data_combined_slow_6\n",
    "data_combined_slow_6_name = \"Dropout 0.9 & L2 1e-3 with 1e-4 learning rate\"\n",
    "print(f\"{data_combined_slow_6_name}: Run time {data_combined_slow_6[2]:.3f} seconds\")\n",
    "print(f\"{data_combined_slow_6_name}: {gen_acc_gap_str(data_combined_slow_6)}\")\n",
    "print(f\"{data_combined_slow_6_name}: {gen_stat_str(data_combined_slow_6)}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41a445ace4f21d9f0dce08bc0cfb89ad306845c53b21585dc74380327349165b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
