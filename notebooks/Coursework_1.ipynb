{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Callable\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hidden_dim = None\n",
    "data_hidden_layer = None\n",
    "\n",
    "data_dropout = None\n",
    "data_l1 = None\n",
    "data_l2 = None\n",
    "\n",
    "data_combined_1 = None\n",
    "data_combined_2 = None\n",
    "data_combined_3 = None\n",
    "data_combined_4 = None\n",
    "data_combined_5 = None\n",
    "data_combined_6 = None\n",
    "data_combined_7 = None\n",
    "data_combined_8 = None\n",
    "\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_plot_stats(\n",
    "    model,\n",
    "    error,\n",
    "    learning_rule,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    num_epochs,\n",
    "    stats_interval,\n",
    "    notebook=True,\n",
    "):\n",
    "\n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors = {\"acc\": lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model,\n",
    "        error,\n",
    "        learning_rule,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        data_monitors,\n",
    "        notebook=notebook,\n",
    "    )\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(\n",
    "        num_epochs=num_epochs, stats_interval=stats_interval\n",
    "    )\n",
    "\n",
    "    # # Plot the change in the validation and training set error over training.\n",
    "    # fig_1 = plt.figure(figsize=(8, 4))\n",
    "    # ax_1 = fig_1.add_subplot(111)\n",
    "    # for k in ['error(train)', 'error(valid)']:\n",
    "    #     ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval,\n",
    "    #               stats[1:, keys[k]], label=k)\n",
    "    # ax_1.legend(loc=0)\n",
    "    # ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # # Plot the change in the validation and training set accuracy over training.\n",
    "    # fig_2 = plt.figure(figsize=(8, 4))\n",
    "    # ax_2 = fig_2.add_subplot(111)\n",
    "    # for k in ['acc(train)', 'acc(valid)']:\n",
    "    #     ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval,\n",
    "    #               stats[1:, keys[k]], label=k)\n",
    "    # ax_2.legend(loc=0)\n",
    "    # ax_2.set_xlabel('Epoch number')\n",
    "\n",
    "    # return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2\n",
    "    return stats, keys, run_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MLP_DATA_DIR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m_/1v_5r42x4dv60qrlc054rmtw0000gn/T/ipykernel_39392/3388012355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Create data provider objects for the MNIST data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/mlpractical/venv/lib/python3.10/site-packages/mlp/data_providers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, which_set, batch_size, max_num_batches, shuffle_order, rng)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# MLP_DATA_DIR environment variable should point to the data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         data_path = os.path.join(\n\u001b[0;32m--> 232\u001b[0;31m             os.environ['MLP_DATA_DIR'], 'emnist-{0}.npz'.format(which_set))\n\u001b[0m\u001b[1;32m    233\u001b[0m         assert os.path.isfile(data_path), (\n\u001b[1;32m    234\u001b[0m             \u001b[0;34m'Data file does not exist at expected path: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.10/3.10.0_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MLP_DATA_DIR'"
     ]
    }
   ],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 11102019 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.layers import DropoutLayer\n",
    "from mlp.penalties import L1Penalty, L2Penalty\n",
    "\n",
    "\n",
    "def run_model(\n",
    "    hidden_dim: int,\n",
    "    hidden_layer_count: int,\n",
    "    dropout_layer: None | Callable[[], DropoutLayer] = None,\n",
    "    penalty: None | Callable[[], L1Penalty | L2Penalty] = None,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Run different models specified in the question.\n",
    "    \"\"\"\n",
    "    if hidden_layer_count < 1:\n",
    "        raise ValueError(\"There must be at least one hidden layer\")\n",
    "\n",
    "    input_dim, output_dim = 784, 47\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.0)\n",
    "\n",
    "    layers: list[object] = [\n",
    "        AffineLayer(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            weights_init,\n",
    "            biases_init,\n",
    "            weights_penalty=penalty,\n",
    "            biases_penalty=penalty,\n",
    "        ),\n",
    "        ReluLayer(),\n",
    "    ]\n",
    "    if dropout_layer is not None:\n",
    "        layers.append(dropout_layer())\n",
    "    for _ in range(hidden_layer_count - 1):\n",
    "        layers += [\n",
    "            AffineLayer(\n",
    "                hidden_dim,\n",
    "                hidden_dim,\n",
    "                weights_init,\n",
    "                biases_init,\n",
    "                weights_penalty=penalty,\n",
    "                biases_penalty=penalty,\n",
    "            ),\n",
    "            ReluLayer(),\n",
    "        ]\n",
    "        if dropout_layer is not None:\n",
    "            layers.append(dropout_layer())\n",
    "    layers.append(\n",
    "        AffineLayer(\n",
    "            hidden_dim,\n",
    "            output_dim,\n",
    "            weights_init,\n",
    "            biases_init,\n",
    "            weights_penalty=penalty,\n",
    "            biases_penalty=penalty,\n",
    "        )\n",
    "    )\n",
    "    model = MultipleLayerModel(layers)\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    # Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    _ = train_model_and_plot_stats(\n",
    "        model,\n",
    "        error,\n",
    "        learning_rule,\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        num_epochs,\n",
    "        stats_interval,\n",
    "        notebook=True,\n",
    "    )\n",
    "    return _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model set up code below is provided as a starting point.\n",
    "# You will probably want to add further code cells for the\n",
    "# different experiments you run.\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "\n",
    "# setup hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "# input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "# weights_init = GlorotUniformInit(rng=rng)\n",
    "# biases_init = ConstantInit(0.)\n",
    "# model = MultipleLayerModel([\n",
    "#     AffineLayer(input_dim, hidden_dim, weights_init, biases_init),\n",
    "#     ReluLayer(),\n",
    "#     AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init),\n",
    "#     ReluLayer(),\n",
    "#     AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "# ])\n",
    "\n",
    "# error = CrossEntropySoftmaxError()\n",
    "# # Use a basic gradient descent learning rule\n",
    "# learning_rule = AdamLearningRule()\n",
    "\n",
    "# #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "# _ = train_model_and_plot_stats(\n",
    "#     model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig_2_3(data: dict[int, Any], prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot utility function for Fig. 2 and 3.\n",
    "    \"\"\"\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "\n",
    "    for i, (k, linestyle) in enumerate({\"(train)\": \"-\", \"(valid)\": \"--\"}.items()):\n",
    "        for w, datum in data.items():\n",
    "            stats: np.ndarray[tuple[int, int], np.dtype[np.float32]] = datum[0]\n",
    "            keys: dict[str, int] = datum[1]\n",
    "            if not i:\n",
    "                # Only print the first time\n",
    "                sorted_keys = sorted(keys.items(), key=itemgetter(1))\n",
    "                stat_str = \", \".join(\n",
    "                    f\"{k[0]}={v:.2e}\" for (k, v) in zip(sorted_keys, stats[-1])\n",
    "                )\n",
    "                print(f\"{w}: {stat_str}\")\n",
    "            ax_1.plot(\n",
    "                np.arange(1, stats.shape[0]) * stats_interval,\n",
    "                stats[1:, keys[f\"error{k}\"]],\n",
    "                label=f\"{prefix} {w}{k}\",\n",
    "                linestyle=linestyle,\n",
    "            )\n",
    "            ax_2.plot(\n",
    "                np.arange(1, stats.shape[0]) * stats_interval,\n",
    "                stats[1:, keys[f\"acc{k}\"]],\n",
    "                label=f\"{prefix} {w}{k}\",\n",
    "                linestyle=linestyle,\n",
    "            )\n",
    "\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel(\"Epoch number\")\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel(\"Epoch number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_hidden_dim is None:\n",
    "    data_hidden_dim = {\n",
    "        32: run_model(32, 1),\n",
    "        64: run_model(64, 1),\n",
    "        128: run_model(128, 1),\n",
    "    }\n",
    "    %store data_hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig_2_3(data_hidden_dim, \"width\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_hidden_layer is None:\n",
    "    data_hidden_layer = {\n",
    "        1: data_hidden_dim[128],\n",
    "        2: run_model(128, 2),\n",
    "        3: run_model(128, 3),\n",
    "    }\n",
    "    %store data_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig_2_3(data_hidden_layer, \"depth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_dropout is None:\n",
    "    data_dropout = {\n",
    "        prob: run_model(128, 3, dropout_layer=lambda: DropoutLayer(incl_prob=prob))\n",
    "        for prob in [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    %store data_dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig_4_dropout(data: dict[float, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Plot utility function for Fig. 2 and 3.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    accs: list[float] = []\n",
    "    gaps: list[float] = []\n",
    "    for dropout, datum in data.items():\n",
    "        stat: np.ndarray[tuple[int], np.dtype[np.float32]] = datum[0][-1]\n",
    "        keys: dict[str, int] = datum[1]\n",
    "        acc = stat[keys[\"acc(valid)\"]]\n",
    "        gap = stat[keys[\"error(valid)\"]] - stat[keys[\"error(train)\"]]\n",
    "        print(f\"Dropout {dropout}: Accuracy {acc:.2e} with gap {gap:.2e}\")\n",
    "        accs.append(acc)\n",
    "        gaps.append(gap)\n",
    "\n",
    "    l1 = ax1.plot(data.keys(), accs, \"r\", label=\"Val. Acc.\")\n",
    "    l2 = ax2.plot(data.keys(), gaps, \"b\", label=\"Gap\")\n",
    "    lines = l1 + l2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "\n",
    "    ax1.legend(lines, labels, loc=0)\n",
    "    ax1.set_xlabel(\"Dropout value\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_ylabel(\"Generalization gap\")\n",
    "\n",
    "\n",
    "plot_fig_4_dropout(data_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_l1 is None:\n",
    "    data_l1 = {\n",
    "        pen: run_model(128, 3, penalty=lambda: L1Penalty(pen))\n",
    "        for pen in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "    %store data_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_l2 is None:\n",
    "    data_l2 = {\n",
    "        pen: run_model(128, 3, penalty=lambda: L2Penalty(pen))\n",
    "        for pen in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    }\n",
    "    %store data_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_combined_1 is None:\n",
    "    data_combined_1 = run_model(\n",
    "        128, 3, dropout_layer=DropoutLayer(incl_prob=0.7), penalty=L2Penalty(1e-3)\n",
    "    )\n",
    "    %store data_combined_1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e81722dedbf8c3dbb7f9f1b508299431c7ae88b04ec4e499defec517632f2d3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
